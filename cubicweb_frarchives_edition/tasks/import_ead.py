# -*- coding: utf-8 -*-
#
# Copyright Â© LOGILAB S.A. (Paris, FRANCE) 2016-2019
# Contact http://www.logilab.fr -- mailto:contact@logilab.fr
#
# This software is governed by the CeCILL-C license under French law and
# abiding by the rules of distribution of free software. You can use,
# modify and/ or redistribute the software under the terms of the CeCILL-C
# license as circulated by CEA, CNRS and INRIA at the following URL
# "http://www.cecill.info".
#
# As a counterpart to the access to the source code and rights to copy,
# modify and redistribute granted by the license, users are provided only
# with a limited warranty and the software's author, the holder of the
# economic rights, and the successive licensors have only limited liability.
#
# In this respect, the user's attention is drawn to the risks associated
# with loading, using, modifying and/or developing or reproducing the
# software by the user in light of its specific status of free software,
# that may mean that it is complicated to manipulate, and that also
# therefore means that it is reserved for developers and experienced
# professionals having in-depth computer knowledge. Users are therefore
# encouraged to load and test the software's suitability as regards their
# requirements in conditions enabling the security of their systemsand/or
# data to be ensured and, more generally, to use and operate it in the
# same conditions as regards security.
#
# The fact that you are presently reading this means that you have had
# knowledge of the CeCILL-C license and that you accept its terms.
#
import logging


import rq

from cubicweb_francearchives import init_bfss, POSTGRESQL_SUPERUSER

from cubicweb_francearchives.dataimport import (
    ead,
    sqlutil,
    es_bulk_index,
    load_services_map,
    service_infos_from_filepath,
)
from cubicweb_francearchives.dataimport.stores import create_massive_store

from cubicweb_frarchives_edition.rq import update_progress, rqjob
from cubicweb_frarchives_edition.tasks.compute_alignments import compute_alignments


def service_code_from_faeid(cnx, faeids):
    eids = ",".join(str(e) for e in faeids)
    rset = cnx.execute(
        "DISTINCT Any C WHERE X is FindingAid, X eid IN ({}), X service S, S code C".format(eids)
    )
    return [c for c, in rset]


def process_import_ead(reader, filepath, services_map, log):
    service_infos = service_infos_from_filepath(filepath, services_map)
    return reader.import_filepath(filepath, service_infos)


@rqjob
def import_ead(
    cnx,
    filepaths,
    auto_dedupe,
    context_service,
    force_delete=False,
    auto_align=True,
    taskeid=None,
):
    launch_task(
        cnx,
        ead.Reader,
        process_import_ead,
        filepaths,
        force_delete=force_delete,
        auto_align=auto_align,
        context_service=context_service,
        auto_dedupe=auto_dedupe,
        taskeid=taskeid,
    )


def launch_task(
    cnx,
    readercls,
    process_func,
    filepaths,
    metadata_filepath=None,
    force_delete=False,
    auto_align=False,
    context_service=True,
    auto_dedupe=True,
    taskeid=None,
):
    config = ead.readerconfig(
        cnx.vreg.config, cnx.vreg.config.appid, esonly=False, nodrop=True, force_delete=force_delete
    )
    log = config["log"] = logging.getLogger("rq.task")
    job = rq.get_current_job()
    current_progress = update_progress(job, 0.0)
    progress_step = 1.0 / (len(filepaths) + 1)
    config["reimport"] = True
    config["nb_processes"] = 1
    config["autodedupe_authorities"] = "{context}/{normalize}".format(
        context="service" if context_service else "global",
        normalize="normalize" if auto_dedupe else "strict",
    )
    foreign_key_tables = sqlutil.ead_foreign_key_tables(cnx.vreg.schema)
    store = create_massive_store(cnx, nodrop=config["nodrop"])
    log.info(
        "Start import: index policy: %s, user: %s",
        config["autodedupe_authorities"],
        "superuser" if POSTGRESQL_SUPERUSER else "no superuser",
    )
    with sqlutil.no_trigger(cnx, foreign_key_tables, interactive=False):
        services_map = load_services_map(cnx)
        init_bfss(cnx.repo)
        indexer = cnx.vreg["es"].select("indexer", cnx)
        es = indexer.get_connection()
        log.info("Getting readercls...")
        r = readercls(config, store)
        failed_importing = []
        for filepath in filepaths:
            log.info("Start importing %r", filepath)
            try:
                if metadata_filepath:
                    # csv import
                    es_docs = process_func(r, filepath, services_map, log, metadata_filepath)
                else:
                    es_docs = process_func(r, filepath, services_map, log)
            except Exception as error:
                es_docs = []
                failed_importing.append(filepath)
                log.exception(
                    """
                Failed to import {fpath} in import_ead task.
                <div class="alert alert-danger">{error}</div>""".format(
                        fpath=filepath, error=error
                    )
                )
            if es_docs:
                log.info("Start reindexing elasticsearch")
                es_bulk_index(es, es_docs)
            log.info("Start flushing massive import")
            store.flush()
            current_progress = update_progress(job, current_progress + progress_step)
        log.info("Start finishing massive import")
        store.finish()
    # remove published findingaid that was deleted in current task
    cnx.system_sql(
        "SELECT published.unpublish_findingaid(fa.cw_eid) "
        "FROM published.cw_findingaid fa LEFT OUTER JOIN entities e ON "
        "(e.eid=fa.cw_eid) WHERE e.eid IS NULL"
    )
    # insert intial state for all FindingAid with no current state
    log.info("Insert intial state for all FindingAid with no current state")
    rset = cnx.execute(
        'Any S WHERE S is State, S state_of WF, X default_workflow WF, X name "FindingAid", '
        "WF initial_state S"
    )
    cnx.system_sql(
        "INSERT INTO in_state_relation (eid_from, eid_to) "
        "SELECT cw_eid, %(eid_to)s FROM cw_findingaid WHERE "
        "NOT EXISTS (SELECT 1 FROM in_state_relation i "
        "WHERE i.eid_from = cw_eid)",
        {"eid_to": rset[0][0]},
    )
    log.info("Imported findingaids number : %r", len(r.imported_findingaids))
    if r.imported_findingaids:
        job = rq.get_current_job()
        if job is not None and taskeid is None:
            taskeid = int(job.id)
        if taskeid is not None:
            entity = cnx.entity_from_eid(taskeid)
            entity.cw_set(fatask_findingaid=r.imported_findingaids)
            log.info("Set %r fatask_findingaid", taskeid)
    cnx.commit()
    if failed_importing:
        log.error(
            f"Import failed for {len(failed_importing)} file. "
            f"Please reimport them : {', '.join( failed_importing)}"
        )

    if not r.imported_findingaids or taskeid is None:
        return
    aligntask = cnx.create_entity(
        "RqTask",
        name="compute_alignments",
        title="automatic compute_alignments for job {}".format(job.id),
    )
    aligntask.cw_adapt_to("IRqJob").enqueue(compute_alignments, r.imported_findingaids, auto_align)
    entity.cw_set(subtasks=aligntask.eid)
    cnx.commit()
