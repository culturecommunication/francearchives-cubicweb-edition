# -*- coding: utf-8 -*-
#
# Copyright Â© LOGILAB S.A. (Paris, FRANCE) 2016-2019
# Contact http://www.logilab.fr -- mailto:contact@logilab.fr
#
# This software is governed by the CeCILL-C license under French law and
# abiding by the rules of distribution of free software. You can use,
# modify and/ or redistribute the software under the terms of the CeCILL-C
# license as circulated by CEA, CNRS and INRIA at the following URL
# "http://www.cecill.info".
#
# As a counterpart to the access to the source code and rights to copy,
# modify and redistribute granted by the license, users are provided only
# with a limited warranty and the software's author, the holder of the
# economic rights, and the successive licensors have only limited liability.
#
# In this respect, the user's attention is drawn to the risks associated
# with loading, using, modifying and/or developing or reproducing the
# software by the user in light of its specific status of free software,
# that may mean that it is complicated to manipulate, and that also
# therefore means that it is reserved for developers and experienced
# professionals having in-depth computer knowledge. Users are therefore
# encouraged to load and test the software's suitability as regards their
# requirements in conditions enabling the security of their systemsand/or
# data to be ensured and, more generally, to use and operate it in the
# same conditions as regards security.
#
# The fact that you are presently reading this means that you have had
# knowledge of the CeCILL-C license and that you accept its terms.
#
"""cubicweb-frarchives_edition application package

Edition components for FranceArchives
"""
import os
import os.path as osp

import re
import urllib.parse

from logilab.common.decorators import monkeypatch

from cubicweb import NoResultError

from cubicweb_varnish.varnishadm import varnish_cli_connect_from_config

from cubicweb.server.hook import DataOperationMixIn, LateOperation
from cubicweb.server.sources import storages
from cubicweb_s3storage import storages as s3_storages

from cubicweb_francearchives import S3_ACTIVE


GEONAMES_RE = re.compile(r"geonames\.org/(\d+)(?:/.+?\.html)?")

CANDIDATE_SEP = "###"

ALIGN_IMPORT_DIR = "/tmp/csv"

FILE_URL_RE = re.compile(r"(^|/)file/(?P<hash>[a-f0-9]{40})/(?P<name>[^/]+)$")

AUTHORITIES = ["LocationAuthority", "SubjectAuthority", "AgentAuthority"]

AUTH_URL_PATTERN = re.compile(
    r".*?((?P<type>location|subject|agent)/)?(?P<eid>\d+)(\?.*)?$"
)  # noqa

SUBJECT_IMAGE_SIZE = (440, 220)


def geonames_id_from_url(geonameuri):
    geonameid = GEONAMES_RE.search(geonameuri)
    if not geonameid:
        return None
    else:
        return geonameid.group(1)


class IncompatibleFile(Exception):
    """Raised when CSV file is incompatible."""


def fspath_for(cnx, eid, attrname="data"):
    """Return the current file-system path for attribute `attrname` of
    entity with `eid`.
    """
    entity = cnx.entity_from_eid(eid)
    storage = cnx.repo.system_source.storage(entity.cw_etype, attrname)
    return storage.current_fs_path(entity, attrname)


@monkeypatch(storages.AddFileOp)
def postcommit_event(self):
    for filepath in self.get_data():
        BfssFinalDeleteFileOperation.get_instance(self.cnx).add_data(("added", filepath))


@monkeypatch(storages.DeleteFileOp)  # noqa
def postcommit_event(self):  # noqa
    for filepath in self.get_data():
        BfssFinalDeleteFileOperation.get_instance(self.cnx).add_data(("deleted", filepath))


_orig_s3_add_postcommit = s3_storages.S3AddFileOp.postcommit_event


@monkeypatch(s3_storages.S3AddFileOp)  # noqa
def postcommit_event(self):  # noqa
    processed = []
    for storage, key, eid, attr in self.get_data():
        if key in processed:
            continue
        processed.append(key)
        S3FinalDeleteFileOperation.get_instance(self.cnx).add_data(
            ("added", storage, key, eid, attr)
        )
    _orig_s3_add_postcommit(self)


@monkeypatch(s3_storages.S3DeleteFileOp)  # noqa
def postcommit_event(self):  # noqa
    for storage, key, eid, attr in self.get_data():
        S3FinalDeleteFileOperation.get_instance(self.cnx).add_data(
            ("deleted", storage, key, eid, attr)
        )


class UnpublishFilesOp(DataOperationMixIn, LateOperation):
    """after a parent entity has been unpublished the fpath / key of related files
    must be removed from the published directory if the path
    is not referenced by any other published entity
    """

    def postcommit_event(self):
        sys_source = self.cnx.repo.system_source
        removed = []
        for parent, cwfile_eid, published_filepath in self.get_data():
            if self.cnx.deleted_in_transaction(cwfile_eid):
                continue
            if published_filepath in removed:
                continue
            # collect all other CWFiles referencing the same filepath (e.g cw_data attribute)
            sql_query = """
            SELECT f.cw_eid FROM cw_file f
            JOIN cw_file f1 ON f.cw_data=f1.cw_data
            WHERE f1.cw_eid =%(eid)s;
            """
            cu = sys_source.doexec(self.cnx, sql_query, {"eid": cwfile_eid})
            cwfiles_eids = [str(f[0]) for f in cu.fetchall()]
            eschema = self.cnx.vreg.schema.eschema("File")
            assert len([e for e in eschema.subjrels if not e.meta and not e.final]) == 0
            # skip 'service_image' relation as irrelevent
            # process 'image_file' relation bellow
            file_objrels = [
                e.type
                for e in eschema.objrels
                if not e.meta and e.type not in ("image_file", "output_file")
            ]
            published_parents_queries = [
                """
                (
                   DISTINCT Any X{i} WHERE X{i} {rel} F,
                   NOT X{i} identity X, X eid %(x)s,
                   X{i} in_state S{i}, S{i} name "%(st)s",
                   F eid IN (%(f)s)
                )
                """.format(
                    i=i, rel=rel
                )
                for i, rel in enumerate(file_objrels)
            ]
            # process 'image_file' relation
            image_eschema = self.cnx.vreg.schema.eschema("Image")
            # Service entity is not IWorkflowable, skip 'service_image' relation
            # SubjectAuthority is not IWorkflowable, skip 'subject_image' relation
            image_objrels = [
                e.type
                for e in image_eschema.objrels
                if not e.meta and e.type not in ("service_image", "subject_image")
            ]
            published_parents_queries.extend(
                [
                    """
                (
                    DISTINCT Any C{i} WHERE
                    I{i} image_file F, C{i} {rel} I{i},
                    NOT C{i} identity X, X eid %(x)s,
                    C{i} in_state CS{i}, CS name "%(st)s",
                    F eid IN (%(f)s)
                )""".format(
                        i=i, rel=rel
                    )
                    for i, rel in enumerate(image_objrels)
                ]
            )
            published_parents_query = "DISTINCT Any X WITH X BEING ({q})".format(
                q=" UNION ".join(published_parents_queries)
            )
            published_parents = self.cnx.execute(
                published_parents_query
                % {"x": parent.eid, "f": ", ".join(cwfiles_eids), "st": "wfs_cmsobject_published"}
            )
            if not published_parents:
                if S3_ACTIVE:
                    self.s3_remove_published_filepath(published_filepath, removed)
                else:
                    self.bfss_remove_published_filepath(published_filepath, removed)

    def s3_remove_published_filepath(self, published_filepath, removed):
        s3storage = self.cnx.repo.system_source.storage("File", "data")
        s3storage.rename_object(published_filepath, ".hidden/" + published_filepath)
        removed.append(published_filepath)

    def bfss_remove_published_filepath(self, published_filepath, removed):
        # removes symbolic link
        os.remove(published_filepath)
        removed.append(published_filepath)


class BfssFinalDeleteFileOperation(DataOperationMixIn, LateOperation):
    def postcommit_event(self):
        fpaths = {"deleted": set(), "added": set()}
        for data in self.get_data():
            fpaths[data[0]].add(data[1])
        deleted = fpaths["deleted"].difference(fpaths["added"])
        pub_appfiles_dir = self.cnx.vreg.config.get("published-appfiles-dir")
        # thoses CWFiles have been deleted. Try to remove the fs stored files
        for filepath in deleted:
            assert isinstance(filepath, str)
            try:
                os.unlink(filepath)
            except Exception as ex:
                self.error("can't remove %s: %s" % (filepath, ex))
            # try to remove the published file even if the previous deletion fails
            if pub_appfiles_dir:
                pub_fpath = osp.join(pub_appfiles_dir, osp.basename(filepath))
                if osp.exists(pub_fpath):
                    try:
                        os.unlink(pub_fpath)
                    except Exception as ex:
                        self.error("can't remove published %s: %s" % (filepath, ex))


class S3FinalDeleteFileOperation(DataOperationMixIn, LateOperation):
    def delete_file(self, storage, key, eid, attr):
        # the code below is a copy of S3AddFileOp.postcommit_event. Asked for
        # refactoring on cubicweb_s3storage
        self.info("Deleting object %s.%s (%s/%s) from S3", eid, attr, storage.bucket, key)
        resp = storage.s3cnx.delete_object(Bucket=storage.bucket, Key=key)
        if resp.get("ResponseMetadata", {}).get("HTTPStatusCode") >= 300:
            self.error("S3 object deletion FAILED: %s", resp)
        else:
            self.debug("S3 object deletion OK: %s", resp)

    def postcommit_event(self):
        fpaths = {"deleted": set(), "added": set()}
        for action_type, storage, key, eid, attr in self.get_data():
            fpaths[action_type].add((key, eid, attr, storage))
        added = [data[0] for data in fpaths["added"]]
        for key, eid, attr, storage in fpaths["deleted"]:
            if key in added:
                continue
            # thoses CWFiles have been deleted. Try to remove the fs stored files
            self.delete_file(storage, key, eid, attr)
            # try to remove the published file even if the previous deletion fails
            if key.startswith(".hidden/"):
                published_key = key.replace(".hidden/", "")
                # test published_key in the bucket : normally should not happen
                if storage.file_exists(published_key):
                    self.delete_file(storage, published_key, eid, attr)


def get_samesas_history(cnx, complete=False):
    if complete:
        query = "SELECT * FROM sameas_history"
    else:
        query = "SELECT sameas_uri, autheid FROM sameas_history"
    return cnx.system_sql(query).fetchall()


def update_samesas_history(cnx, records):
    """
    Update `samesas_history` sql table with entites created ou deleted by users

    Args:

    - `records` is a list with a 3-tuple list:
      - sameas_uri: uri of related ExternalUri
      - autheid: eid of related Authority
      - action: 0 or 1:
         - 1 : created entity
         - 0 : deleted entity
    """
    records = [
        {"sameas_uri": sauri, "autheid": autheid, "action": act} for sauri, autheid, act in records
    ]
    cnx.cnxset.cu.executemany(
        """
        INSERT INTO sameas_history (sameas_uri, autheid, action)
        VALUES (%(sameas_uri)s, %(autheid)s, %(action)s::boolean)
        ON CONFLICT (sameas_uri, autheid) DO UPDATE SET action = EXCLUDED.action
    """,
        records,
    )


def compute_leaflet_json(cnx):
    """compute json for leaflet"""
    json_data = {"cms": [], "consultation": []}
    rset = cnx.execute(
        "Any P, PL, PLAT, PLNG, COUNT(F) GROUPBY P, PL, PLAT, PLNG "
        "WHERE I authority P, I index F, "
        "P is LocationAuthority, P latitude PLAT, P longitude PLNG, "
        "P label PL, NOT P latitude NULL"
    )
    consult_base_url = cnx.vreg.config.get("consultation-base-url")
    for eid, label, lat, lng, count in rset:
        json_data["cms"].append(
            {
                "eid": eid,
                "label": label,
                "lat": lat,
                "lng": lng,
                "dashLabel": "--" in label,
                "count": count,
                "url": cnx.build_url("location/{}".format(eid)),
            }
        )
        json_data["consultation"].append(
            {
                "eid": eid,
                "label": label,
                "lat": lat,
                "lng": lng,
                "dashLabel": "--" in label,
                "count": count,
                "url": "{}/location/{}".format(consult_base_url, eid),
            }
        )
    return json_data


def get_leaflet_cache_entities(cnx):
    """access to leaflet json stored in database"""
    try:
        return cnx.execute(
            "Any X, V WHERE X is Caches, X name %(name)s, X values V", {"name": "geomap"}
        )
    except NoResultError:
        return None


def load_leaflet_json(cnx):
    """load json data for leaflet map"""
    json_data = compute_leaflet_json(cnx)
    cache_name = "geomap"
    for instance_type in ("cms", "consultation"):
        try:
            cache = cnx.execute(
                """Any X, V WHERE X is Caches,
                   X instance_type %(instance_type)s,
                   X name %(name)s, X values V""",
                {"name": cache_name, "instance_type": instance_type},
            ).one()
        except NoResultError:
            cache = cnx.create_entity("Caches", name=cache_name, instance_type=instance_type)
        cache.cw_set(values=json_data[instance_type])


class ForbiddenPublishedTransition(Exception):
    """Asked Transaction can not be fired"""

    def __init__(self, cnx, msg):
        super(ForbiddenPublishedTransition, self).__init__({cnx._("Impossible to publish"): msg})


class VarnishPurgeMixin(object):
    def purge_varnish(self, urls, config):
        if not config.get("varnishcli-hosts"):
            return
        purge_cmd = "ban req.url ~"
        cnxs = varnish_cli_connect_from_config(config)
        for url in urls:
            for varnish_cli in cnxs:
                varnish_cli.execute(purge_cmd, "^%s" % urllib.parse.urlparse(url).path)
        for varnish_cli in cnxs:
            varnish_cli.close()


def update_suggest_es(cnx, entities):
    if cnx.vreg.config.mode == "test":
        return
    service = cnx.vreg["services"].select("reindex-suggest", cnx)
    try:
        service.index_authorities(entities)
    except Exception:
        import traceback

        traceback.print_exc()
